{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6814705b-48ed-41ee-b90f-81047ea0ee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate>=0.26.0 datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7580b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.088458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.083800</td>\n",
       "      <td>1.071702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.083800</td>\n",
       "      <td>1.041938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent model fine-tuning completed and saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.122265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.598600</td>\n",
       "      <td>4.104264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.598600</td>\n",
       "      <td>4.075503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA model fine-tuning completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from uuid import uuid4\n",
    "import transformers\n",
    "\n",
    "# Check transformers version for compatibility\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Initialize tokenizer and models\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "intent_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "qa_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "intent_labels = [\"greeting\", \"small_talk\", \"symptom_query\"]\n",
    "\n",
    "# Custom Dataset for Intent Classification\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Custom Dataset for Question Answering\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, contexts, start_positions, end_positions):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.start_positions = start_positions\n",
    "        self.end_positions = end_positions\n",
    "        self.encodings = tokenizer(questions, contexts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"start_positions\"] = torch.tensor(self.start_positions[idx])\n",
    "        item[\"end_positions\"] = torch.tensor(self.end_positions[idx])\n",
    "        return item\n",
    "\n",
    "# Load and prepare data\n",
    "def load_data():\n",
    "    with open(\"symptom_data.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Intent classification data\n",
    "    intent_texts = []\n",
    "    intent_labels_idx = []\n",
    "    for intent in data[\"intents\"]:\n",
    "        for example in intent[\"examples\"]:\n",
    "            intent_texts.append(example)\n",
    "            intent_labels_idx.append(intent_labels.index(intent[\"type\"]) if intent[\"type\"] in intent_labels else 2)\n",
    "\n",
    "    # Question-answering data (extended for QA format)\n",
    "    qa_questions = []\n",
    "    qa_contexts = []\n",
    "    qa_start_positions = []\n",
    "    qa_end_positions = []\n",
    "    for intent in data[\"intents\"]:\n",
    "        if intent[\"type\"] == \"symptom_query\":\n",
    "            context = intent[\"response\"]\n",
    "            for example in intent[\"examples\"]:\n",
    "                qa_questions.append(example)\n",
    "                qa_contexts.append(context)\n",
    "                # Approximate start and end positions for the answer (simplified)\n",
    "                tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "                answer_start = 0  # Start of the context (simplified for demo)\n",
    "                answer_end = len(tokens) - 1\n",
    "                qa_start_positions.append(answer_start)\n",
    "                qa_end_positions.append(answer_end)\n",
    "\n",
    "    return intent_texts, intent_labels_idx, qa_questions, qa_contexts, qa_start_positions, qa_end_positions\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_models():\n",
    "    # Load data\n",
    "    intent_texts, intent_labels_idx, qa_questions, qa_contexts, qa_start_positions, qa_end_positions = load_data()\n",
    "\n",
    "    # Split data for intent classification\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        intent_texts, intent_labels_idx, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Split data for question answering\n",
    "    qa_train_indices, qa_val_indices = train_test_split(\n",
    "        range(len(qa_questions)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_qa_questions = [qa_questions[i] for i in qa_train_indices]\n",
    "    val_qa_questions = [qa_questions[i] for i in qa_val_indices]\n",
    "    train_qa_contexts = [qa_contexts[i] for i in qa_train_indices]\n",
    "    val_qa_contexts = [qa_contexts[i] for i in qa_val_indices]\n",
    "    train_qa_start_positions = [qa_start_positions[i] for i in qa_train_indices]\n",
    "    val_qa_start_positions = [qa_start_positions[i] for i in qa_val_indices]\n",
    "    train_qa_end_positions = [qa_end_positions[i] for i in qa_train_indices]\n",
    "    val_qa_end_positions = [qa_end_positions[i] for i in qa_val_indices]\n",
    "\n",
    "    # Create datasets\n",
    "    train_intent_dataset = IntentDataset(train_texts, train_labels)\n",
    "    val_intent_dataset = IntentDataset(val_texts, val_labels)\n",
    "    train_qa_dataset = QADataset(train_qa_questions, train_qa_contexts, train_qa_start_positions, train_qa_end_positions)\n",
    "    val_qa_dataset = QADataset(val_qa_questions, val_qa_contexts, val_qa_start_positions, val_qa_end_positions)\n",
    "\n",
    "    # Training arguments for intent model\n",
    "    intent_training_args = TrainingArguments(\n",
    "        output_dir=\"./intent_results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./intent_logs\",\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Trainer for intent model\n",
    "    intent_trainer = Trainer(\n",
    "        model=intent_model,\n",
    "        args=intent_training_args,\n",
    "        train_dataset=train_intent_dataset,\n",
    "        eval_dataset=val_intent_dataset,\n",
    "    )\n",
    "\n",
    "    # Train intent model\n",
    "    intent_trainer.train()\n",
    "    intent_model.save_pretrained(\"fine_tuned_intent_model\")\n",
    "    tokenizer.save_pretrained(\"fine_tuned_intent_model\")\n",
    "    print(\"Intent model fine-tuning completed and saved.\")\n",
    "\n",
    "    # Training arguments for QA model\n",
    "    qa_training_args = TrainingArguments(\n",
    "        output_dir=\"./qa_results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./qa_logs\",\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Trainer for QA model\n",
    "    qa_trainer = Trainer(\n",
    "        model=qa_model,\n",
    "        args=qa_training_args,\n",
    "        train_dataset=train_qa_dataset,\n",
    "        eval_dataset=val_qa_dataset,  # Added evaluation dataset\n",
    "    )\n",
    "\n",
    "    # Train QA model\n",
    "    qa_trainer.train()\n",
    "    qa_model.save_pretrained(\"fine_tuned_qa_model\")\n",
    "    tokenizer.save_pretrained(\"fine_tuned_qa_model\")\n",
    "    print(\"QA model fine-tuning completed and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fine_tune_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Intent dataset distribution: {'greeting': 0, 'small_talk': 3, 'symptom_query': 49, 'out_of_scope': 2}\n",
      "WARNING:__main__:Too few greeting examples. Adding more examples.\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1768.96 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 789.65 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Starting intent model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.205900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Intent model training completed.\n",
      "INFO:__main__:Starting QA model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 06:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.135200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:QA model training completed.\n",
      "INFO:__main__:Model saved successfully to fine_tuned_intent_model\n",
      "INFO:__main__:Removing existing directory: fine_tuned_intent_model\n",
      "INFO:__main__:Model saved successfully to fine_tuned_intent_model\n",
      "INFO:__main__:Model saved successfully to fine_tuned_qa_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load symptom data\n",
    "try:\n",
    "    with open(\"symptom_data.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"symptom_data.json not found in the current directory\")\n",
    "    raise\n",
    "\n",
    "# Define intent labels\n",
    "intent_labels = [\"greeting\", \"small_talk\", \"symptom_query\", \"out_of_scope\"]\n",
    "\n",
    "# Prepare dataset for intent classification\n",
    "intent_texts = []\n",
    "intent_labels_idx = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for example in intent[\"examples\"]:\n",
    "        intent_texts.append(example)\n",
    "        intent_labels_idx.append(intent_labels.index(intent[\"type\"]) if intent[\"type\"] in intent_labels else 3)\n",
    "\n",
    "# Check dataset balance\n",
    "label_counts = {label: intent_labels_idx.count(i) for i, label in enumerate(intent_labels)}\n",
    "logger.info(f\"Intent dataset distribution: {label_counts}\")\n",
    "if label_counts[\"greeting\"] < 5:\n",
    "    logger.warning(\"Too few greeting examples. Adding more examples.\")\n",
    "    intent_texts.extend([\"hello there\", \"hi bot\", \"greetings\", \"hey bot\", \"good morning\"])\n",
    "    intent_labels_idx.extend([0, 0, 0, 0, 0])\n",
    "\n",
    "# Prepare dataset for question answering\n",
    "qa_questions = []\n",
    "qa_contexts = []\n",
    "qa_answers = []\n",
    "for intent in data[\"intents\"]:\n",
    "    if intent[\"type\"] == \"symptom_query\":\n",
    "        for example in intent[\"examples\"]:\n",
    "            qa_questions.append(example)\n",
    "            qa_contexts.append(intent[\"response\"])\n",
    "            answer_text = intent[\"response\"].split(\".\")[0]\n",
    "            start_pos = intent[\"response\"].find(answer_text)\n",
    "            qa_answers.append({\"text\": answer_text, \"start\": start_pos, \"end\": start_pos + len(answer_text)})\n",
    "\n",
    "# Create datasets\n",
    "intent_dataset = Dataset.from_dict({\"text\": intent_texts, \"label\": intent_labels_idx})\n",
    "qa_dataset = Dataset.from_dict({\"question\": qa_questions, \"context\": qa_contexts, \"answers\": qa_answers})\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Tokenize intent classification dataset\n",
    "def tokenize_intent(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "intent_dataset = intent_dataset.map(tokenize_intent, batched=True)\n",
    "intent_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Tokenize QA dataset\n",
    "def tokenize_qa(examples):\n",
    "    encodings = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offset in enumerate(encodings[\"offset_mapping\"]):\n",
    "        start_char = examples[\"answers\"][i][\"start\"]\n",
    "        end_char = examples[\"answers\"][i][\"end\"]\n",
    "        start_pos = None\n",
    "        end_pos = None\n",
    "        for j, (start, end) in enumerate(offset):\n",
    "            if start <= start_char < end:\n",
    "                start_pos = j\n",
    "            if start < end_char <= end:\n",
    "                end_pos = j\n",
    "        start_positions.append(start_pos if start_pos is not None else 0)\n",
    "        end_positions.append(end_pos if end_pos is not None else 0)\n",
    "    encodings[\"start_positions\"] = start_positions\n",
    "    encodings[\"end_positions\"] = end_positions\n",
    "    return encodings\n",
    "\n",
    "qa_dataset = qa_dataset.map(tokenize_qa, batched=True)\n",
    "qa_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    intent_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(intent_labels))\n",
    "    qa_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading models: {e}\")\n",
    "    raise\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,  # Increased for better convergence\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize trainers\n",
    "intent_trainer = Trainer(\n",
    "    model=intent_model,\n",
    "    args=training_args,\n",
    "    train_dataset=intent_dataset,\n",
    ")\n",
    "\n",
    "qa_trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    args=training_args,\n",
    "    train_dataset=qa_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune models\n",
    "try:\n",
    "    logger.info(\"Starting intent model training...\")\n",
    "    intent_trainer.train()\n",
    "    logger.info(\"Intent model training completed.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during intent model training: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting QA model training...\")\n",
    "    qa_trainer.train()\n",
    "    logger.info(\"QA model training completed.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during QA model training: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save fine-tuned models with error handling\n",
    "def save_model_with_retry(model, save_path, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if os.path.exists(save_path):\n",
    "                logger.info(f\"Removing existing directory: {save_path}\")\n",
    "                shutil.rmtree(save_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            model.save_pretrained(save_path)\n",
    "            logger.info(f\"Model saved successfully to {save_path}\")\n",
    "            return\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Attempt {attempt + 1} failed to save model to {save_path}: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                raise OSError(f\"Failed to save model to {save_path} after {retries} attempts: {e}\")\n",
    "print(intent_texts, intent_labels_idx)\n",
    "try:\n",
    "    save_model_with_retry(intent_model, \"fine_tuned_intent_model\")\n",
    "    save_model_with_retry(tokenizer, \"fine_tuned_intent_model\")\n",
    "    save_model_with_retry(qa_model, \"fine_tuned_qa_model\")\n",
    "except OSError as e:\n",
    "    logger.error(f\"Error saving models: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
